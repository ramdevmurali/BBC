# HM Land Registry: A Critical Analysis of NLP Model Performance

**Candidate:** Ramdev Murali
**Date:** July 2, 2025

---

## 1. Project Mandate & Executive Summary

This repository documents a systematic investigation into solving a real-world text classification problem, as specified by the HM Land Registry NLP Challenge. The project's mandate was not merely to achieve a high accuracy score, but to explore the trade-offs between modern NLP techniques, from rapid, flexible baselining to high-performance, specialized fine-tuning.

The project was executed in two distinct, strategic phases:

1.  **Phase I: Zero-Shot Baseline & Feasibility Analysis:** A professional application prototype was built using a general-purpose Zero-Shot model. This established a robust initial performance baseline of **60% accuracy** and allowed for a deep analysis of the dataset's inherent ambiguities and complexities.

2.  **Phase II: State-of-the-Art Fine-Tuning & Performance Mastery:** To test the upper limits of model capability, a `DistilBERT` model was fine-tuned on the specific dataset. This resource-intensive task, performed on a cloud GPU, resulted in a hyper-specialized model achieving **~100% accuracy** on the held-out test set.

The final conclusion is a nuanced one: while near-perfect accuracy is achievable, its "brittleness" must be understood. The true value lies in the journey between the 60% baseline and the 100% mastery, revealing critical insights into model selection, domain adaptation, and production-readiness.

---

## 2. Technical Stack & Engineering Principles

This project adheres to professional software engineering and MLOps principles.

-   **Languages & Libraries:** Python 3.10+, PyTorch, Hugging Face `transformers`, `datasets`, `evaluate`, Scikit-learn, Pandas.
-   **Architecture:**
    -   **Application (`/src`):** Modular, decoupled logic with centralized configuration. Designed for stability and inference.
    -   **Experimentation (`/notebooks`):** An interactive Colab notebook for GPU-accelerated model training and exploratory data analysis.
-   **Reproducibility:** The environment is fully specified via `requirements.txt` and managed with `venv`.

---

## 3. Operational Guide: Reproducing the Results

To replicate this project, follow these steps.

1.  **Clone the Repository & Set Up Environment:**
    ```bash
    git clone https://github.com/ramdevmurali/HMLR.git
    cd HMLR
    python3 -m venv venv
    source venv/bin/activate
    pip install -r requirements.txt
    ```

2.  **Run the Baseline Application:**
    This command executes the original prototype using the flexible Zero-Shot model.
    ```bash
    python -m src.main
    ```

3.  **Review the High-Performance Model Training:**
    The notebook `notebooks/Fine_Tuning_and_Evaluation.ipynb` contains the full code and logs for the fine-tuning process that produced the state-of-the-art model.

> **Note:** The fine-tuned model artifacts are not included in this repository (per `.gitignore` best practices for large files) but can be regenerated by running the notebook on a GPU-enabled environment like Google Colab.

---

## 4. Performance Deep Dive: A Tale of Two Models

### Phase I: The 60% Accuracy Baseline (Zero-Shot Model)

The initial approach used a generalist Zero-Shot model. This provided an immediate, valuable baseline and, more importantly, a diagnostic tool.

-   **Result:** A **60% accuracy**, while modest, is **300% better than random chance**, proving the model could extract a significant signal from the text.
-   **Key Insight:** The model's confusion matrix revealed the inherent ambiguities in the dataset itselfâ€”specifically, the semantic overlap between categories like `tech` and `business`. This finding was crucial, proving that a "perfect" score might be misleading without understanding the data's nature.

### Phase II: The ~100% Accuracy Mastery (Fine-Tuned Model)

To achieve maximum performance, a `DistilBERT` model was fine-tuned specifically on the BBC training data.

-   **Result:** The model achieved near-perfect scores across all metrics on the unseen test set.

**Final Classification Report (Fine-Tuned Model):**
```
               precision    recall  f1-score   support

     business       1.00      1.00      1.00        51
entertainment       0.97      1.00      0.99        39
     politics       1.00      0.98      0.99        42
        sport       1.00      1.00      1.00        51
         tech       1.00      1.00      1.00        40

     accuracy                           1.00       223
    macro avg       0.99      1.00      1.00       223
 weighted avg       1.00      1.00      1.00       223
```

**Analysis of the ~100% Score:**
This near-perfect result is not interpreted as simple "success" but rather as evidence of **hyper-specialization**. The model has mastered the specific linguistic patterns, vocabulary, and stylistic quirks of the *2005 BBC News corpus*. While it has successfully generalized to the held-out test set from the *same distribution*, this mastery is considered "brittle."

![Final Confusion Matrix](outputs/finetuned_confusion_matrix.png)

---

## 5. Production Readiness & Next Steps

Based on this analysis, the fine-tuned model, despite its score, is not immediately production-ready. The critical next steps would be:

1.  **Test for Domain Shift:** The highest priority would be to evaluate the model's performance on out-of-distribution data. This includes articles from a different publisher (e.g., The Guardian) or a different time period (e.g., 2024 news). This would expose the brittleness of its current "mastery" and provide a true measure of its real-world generalization.
2.  **Develop a Re-training Strategy:** Based on the results of domain shift testing, a strategy for continuous monitoring and periodic re-training would be designed to ensure the model remains accurate as news trends and language evolve.
3.  **Integrate and Deploy:** The finalized model would be integrated into the robust, modular `/src` application, containerized (e.g., with Docker), and deployed as a scalable inference service.

This rigorous, two-phase approach provides a complete picture of the model's capabilities and limitations, paving a clear and responsible path toward production deployment.