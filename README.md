# HM Land Registry: A Critical Analysis of NLP Model Performance

**Candidate:** Ramdev Murali
**Date:** July 2, 2025

---

## 1. Executive Summary

This repository documents a systematic investigation into solving a real-world text classification problem, as specified by the HM Land Registry NLP Challenge. The project's mandate was not merely to achieve a high accuracy score, but to explore the trade-offs between modern NLP techniques, from rapid, flexible baselining to high-performance, specialized fine-tuning.

The project successfully delivers on all core requirements, culminating in a state-of-the-art model with **~100% accuracy** on the primary classification task. This multi-stage approach demonstrates a robust skillset in model selection, rapid prototyping, performance optimization, and professional software engineering practices.

---

## 2. Technical Stack & Key Concepts

-   **Language:** Python 3.10+
-   **Core Libraries:** `PyTorch`, `Hugging Face Transformers`, `Hugging Face Datasets`, `evaluate`, `Scikit-learn`, `Pandas`.
-   **Key Concepts Demonstrated:** Transfer Learning, Zero-Shot Classification, Fine-Tuning, Model Evaluation, MLOps (Reproducible Environments, Version Control).
-   **Development Environments:** Local `venv` for application logic, Google Colab with GPU for intensive model training.

---

## 3. Operational Guide: Reproducing the Results

To replicate this project locally, please follow these steps.

1.  **Clone the Repository & Set Up Environment:**
    ```bash
    git clone https://github.com/ramdevmurali/HMLR.git
    cd HMLR
    python3 -m venv venv
    source venv/bin/activate
    pip install -r requirements.txt
    ```

2.  **Run the Baseline Application:**
    This command executes the original prototype which fulfills all the specified tasks using the Zero-Shot model.
    ```bash
    python -m src.main
    ```

3.  **Review the High-Performance Model Training:**
    The notebook `notebooks/Fine_Tuning_and_Evaluation.ipynb` contains the full code and logs for the fine-tuning process that produced the state-of-the-art classification model.

> **Note:** The fine-tuned model artifacts are not included in this repository (per `.gitignore` best practices for large files) but can be regenerated by running the notebook on a GPU-enabled environment like Google Colab.

---

## 4. Task Fulfillment & Deliverables

This project successfully addresses all tasks outlined in the challenge document. This section explicitly maps the project's features to those original requirements.

#### **✅ Essential Task: Sub-Category Classification**
-   **Requirement:** *"Use the full text dataset and classify each existing category into sub-categories."*
-   **Status:** **Fulfilled.**
-   **Implementation:** This is performed by the Zero-Shot model in the main application (`src/main.py`).
-   **Deliverable:** The `outputs/classification_results.csv` file.

#### **✅ Desired Task 1: Named Entity Recognition**
-   **Requirement:** *"Identify documents and extract the named entities for media personalities, clearly identifying their jobs."*
-   **Status:** **Partially Fulfilled.**
-   **Implementation:** The NER model successfully extracts `PERSON` entities. The more complex sub-task of relation extraction (identifying jobs) was deemed out of scope for this prototype, as encouraged by the challenge documentation for difficult tasks.
-   **Deliverable:** The `outputs/ner_results.csv` file demonstrates the successful entity extraction.

#### **✅ Desired Task 2: Event Summarization**
-   **Requirement:** *"Extract summaries of anything that took place or is/was scheduled to take place in April."*
-   **Status:** **Fulfilled.**
-   **Implementation:** The main pipeline filters for articles containing "April" and uses a generative summarizer.
-   **Deliverable:** The `outputs/summarization_results.csv` file.

---

## 5. Architectural & Model Strategy

A "right tool for the job" philosophy was adopted, balancing performance with pragmatism.

-   **Application Structure:** The project is built on a modular architecture (`/src`) to ensure stability and maintainability. Experimentation and model training are separated into a `/notebooks` directory, mirroring professional MLOps workflows.

-   **Model Selection Strategy:**
    -   **For Classification:** To achieve maximum performance on the core task, a `DistilBERT` model was **fine-tuned** to create a hyper-specialized expert for this specific dataset.
    -   **For NER & Summarization:** To deliver these features efficiently, proven, **pre-specialized models** were leveraged. This included an off-the-shelf NER model (`dslim/bert-base-NER`) and a generative encoder-decoder model for summarization, which is architecturally suited for the task. This pragmatic approach avoids redundant training and demonstrates efficient use of existing state-of-the-art tools.

---

## 6. Performance Deep Dive: The Fine-Tuned Model

After establishing a 60% accuracy baseline with a Zero-Shot model, the fine-tuned `DistilBERT` achieved **~100% accuracy** on the unseen test set, effectively solving the classification task.

**Final Classification Report (Fine-Tuned Model):**
```
               precision    recall  f1-score   support

     business       1.00      1.00      1.00        51
entertainment       0.97      1.00      0.99        39
     politics       1.00      0.98      0.99        42
        sport       1.00      1.00      1.00        51
         tech       1.00      1.00      1.00        40

     accuracy                           1.00       223
    macro avg       0.99      1.00      1.00       223
 weighted avg       1.00      1.00      1.00       223
```

**Analysis of the ~100% Score:**
This near-perfect result is interpreted not as simple success, but as evidence of **hyper-specialization**. The model has mastered the specific linguistic patterns of the *2005 BBC News corpus*. While it has successfully generalized to the held-out test set from the *same distribution*, this mastery is considered "brittle."

![Final Confusion Matrix](outputs/finetuned_confusion_matrix.png)

---

## 7. Production Readiness & Next Steps

Based on this analysis, the fine-tuned model, despite its score, is not immediately production-ready. The critical next steps would be:

1.  **Test for Domain Shift:** Evaluate the model on out-of-distribution data (e.g., news from 2025 or from a different publisher) to measure its true real-world generalization.
2.  **Develop a Re-training Strategy:** Design a strategy for continuous monitoring and periodic re-training to ensure the model remains accurate as language and topics evolve.
3.  **Deploy as a Service:** Integrate the finalized model into the robust `/src` application, containerize it (e.g., with Docker), and deploy it as a scalable inference API.