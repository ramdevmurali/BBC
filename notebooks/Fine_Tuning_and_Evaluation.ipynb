{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmhvLdz5Psw3"
      },
      "outputs": [],
      "source": [
        "# Cell 1 (The Definitive Install)\n",
        "!pip install \"transformers[torch]>=4.38\" \"datasets>=2.18\" \"evaluate>=0.4\" scikit-learn pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FWuc7moMQgqw"
      },
      "outputs": [],
      "source": [
        "!unzip \"BBC Full Text.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAWP-nVDRA-c"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Load data into a pandas DataFrame\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "data_path = 'bbc/'\n",
        "data = []\n",
        "\n",
        "# Loop through each category folder\n",
        "for category in os.listdir(data_path):\n",
        "    category_path = os.path.join(data_path, category)\n",
        "    if os.path.isdir(category_path):\n",
        "        # Loop through each text file\n",
        "        for filename in os.listdir(category_path):\n",
        "            if filename.endswith('.txt'):\n",
        "                file_path = os.path.join(category_path, filename)\n",
        "                with open(file_path, 'r', encoding='latin-1') as f:\n",
        "                    text = f.read()\n",
        "                    # Append a dictionary for this article to our list\n",
        "                    data.append({'text': text, 'label_name': category})\n",
        "\n",
        "# Create the DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# IMPORTANT: Shuffle the dataframe to ensure randomness\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Data loaded successfully!\")\n",
        "print(\"Shape of DataFrame:\", df.shape)\n",
        "print(\"\\nSample of the data:\")\n",
        "print(df.head())\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(df['label_name'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoBSXpk_RO3A"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Create label mappings and split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a sorted list of unique labels to ensure consistent mapping\n",
        "labels = sorted(df['label_name'].unique().tolist())\n",
        "label2id = {label: i for i, label in enumerate(labels)}\n",
        "id2label = {i: label for i, label in enumerate(labels)}\n",
        "\n",
        "# Add the integer 'label' column to our DataFrame\n",
        "df['label'] = df['label_name'].map(label2id)\n",
        "\n",
        "print(\"Label to ID Mapping:\")\n",
        "print(label2id)\n",
        "print(\"\\nID to Label Mapping:\")\n",
        "print(id2label)\n",
        "\n",
        "# Split the data: 90% for training, 10% for testing\n",
        "# 'stratify' ensures that both sets have a similar proportion of each category\n",
        "train_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.1,\n",
        "    random_state=42,\n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "print(\"\\nTrain set size:\", len(train_df))\n",
        "print(\"Test set size:\", len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwl0-1cpRcLn"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Convert pandas DataFrames to Hugging Face Datasets\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Convert our training and testing DataFrames into Dataset objects\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Combine them into a single DatasetDict object for easy management\n",
        "raw_datasets = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "# You can inspect the new object\n",
        "print(raw_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_SjrhCEReDr"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Tokenize the datasets\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Define the model we are going to fine-tune\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "# Load its corresponding tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Define a function that will apply the tokenizer to a batch of examples\n",
        "def tokenize_function(examples):\n",
        "    # The tokenizer will pad shorter texts and truncate longer ones to a standard length\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Use the .map() method to apply this function to all examples in our datasets.\n",
        "# batched=True makes the process much faster.\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "# Let's inspect the result. You'll see new columns like 'input_ids' and 'attention_mask'.\n",
        "print(tokenized_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M9fpVYEUbZE"
      },
      "outputs": [],
      "source": [
        "# Cell 7 (Bare Bones - Guaranteed to be Compatible)\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "# --- 1. Load the pre-trained model ---\n",
        "# This part is correct and remains the same.\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=len(labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# --- 2. Define Bare-Minimum Training Arguments ---\n",
        "# We are removing EVERY optional argument that could cause an error.\n",
        "# This must be compatible with any version of the library.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"training_output\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16\n",
        ")\n",
        "\n",
        "# --- 3. Create a Simplified Trainer ---\n",
        "# We are REMOVING the compute_metrics function for now.\n",
        "# We will evaluate manually AFTER training is done.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    # We still provide the test set here for later.\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# --- 4. Train the model! ---\n",
        "print(\"Starting fine-tuning with a simplified configuration...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-sGixdcV-y7"
      },
      "outputs": [],
      "source": [
        "# Cell 8: Final Evaluation\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"Running final evaluation on the test set...\")\n",
        "\n",
        "# Use the trainer to make predictions on our test dataset\n",
        "predictions_output = trainer.predict(tokenized_datasets[\"test\"])\n",
        "\n",
        "# The output contains the raw model outputs (logits). We need to find the highest score for each.\n",
        "predicted_labels = np.argmax(predictions_output.predictions, axis=1)\n",
        "# These are the correct answers\n",
        "true_labels = tokenized_datasets[\"test\"][\"label\"]\n",
        "\n",
        "# --- Generate Classification Report ---\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "# We use our `id2label` mapping to show the actual category names\n",
        "report = classification_report(true_labels, predicted_labels, target_names=[id2label[i] for i in range(len(id2label))])\n",
        "print(report)\n",
        "\n",
        "\n",
        "# --- Generate Confusion Matrix ---\n",
        "print(\"\\n--- Generating Confusion Matrix ---\")\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[id2label[i] for i in range(len(id2label))], yticklabels=[id2label[i] for i in range(len(id2label))])\n",
        "plt.title('Confusion Matrix: Fine-Tuned DistilBERT')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "\n",
        "# Save the plot to the Colab environment\n",
        "plt.savefig(\"finetuned_confusion_matrix.png\")\n",
        "print(\"\\nConfusion matrix saved as finetuned_confusion_matrix.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unDI185hWqR_"
      },
      "outputs": [],
      "source": [
        "# Cell 9: Save Model and Zip Results\n",
        "\n",
        "# Define a path to save the final model\n",
        "final_model_path = \"bbc-distilbert-finetuned\"\n",
        "# Save the trained model to this path\n",
        "trainer.save_model(final_model_path)\n",
        "print(f\"Final model saved to '{final_model_path}'\")\n",
        "\n",
        "# Create a single ZIP file containing the model and the confusion matrix image\n",
        "!zip -r results.zip bbc-distilbert-finetuned finetuned_confusion_matrix.png\n",
        "\n",
        "print(\"\\nAll artifacts zipped into 'results.zip'. Please download this file.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}